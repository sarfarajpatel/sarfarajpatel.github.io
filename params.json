{"name":"Sarfarajpatel.github.io","tagline":"","body":"##The goal of this machine learning exercise is to predict the manner in which the participants did the exercise–that is, to #predict the “classe” variable found in the training set. The prediction model will then be used to predict twenty different #test cases, \r\n##as provided in the testing dataset.\r\n\r\n##Begin by loading the required libraries and reading in the training and testing datasets, assigning missing values to entries #that are currently \r\n##'NA' or blank.\r\n\r\nlibrary(corrplot)\r\nlibrary(caret)\r\n\r\nwm <- read.csv(\"pml-training.csv\", header = TRUE, na.strings = c(\"NA\", \"\"))\r\nwm_test <- read.csv(\"pml-testing.csv\", header = TRUE, na.strings = c(\"NA\", \"\"))\r\n\r\n##Columns in the orignal training and testing datasets that are mostly filled with missing values are then removed. To do this, #count the number of missing values in each column of the full training dataset. We use those sums to create a logical variable #for each column of the dataset. \r\n##The logical variable's value is 'TRUE' if a column has no missing values (i.e. if the colSums = 0). If there are missing #values in the column, the logical variable's value corresponding to that column will be 'FALSE'.\r\n\r\n##Applying the logical variable to the columns of the training and testing datasets will only keep those columns that are \r\n##complete. (Note: This is a way of applying the 'complete.cases' function to the columns of a dataset.)\r\n\r\n##Our updated training dataset now has fewer variables to review in our analysis. Further, our final testing dataset has \r\n##consistent columns in it. This will allow the fitted model to be applied to the testing dataset.\r\n\r\ncsums <- colSums(is.na(wm))\r\n\r\ncsums_log <- (csums == 0)\r\n\r\ntraining_fewer_cols <- wm[, (colSums(is.na(wm)) == 0)]\r\n\r\nwm_test <- wm_test[, (colSums(is.na(wm)) == 0)]\r\n\r\n\r\n\r\n##Create another logical vector in order to delete additional unnecessary columns from the pared-down training and testing\r\n##datasets. Column names in the dataset containing the entries shown in the 'grepl' function will have a value of 'TRUE' \r\n##in the logical vector. Since these are the columns we want to remove, we apply the negation of the logical vector against \r\n##the columns of our dataset.\r\n\r\ndel_cols_log <- grepl(\"X|user_name|timestamp|new_window\", colnames(training_fewer_cols))\r\n\r\ntraining_fewer_cols <- training_fewer_cols[, !del_cols_log]\r\n\r\nwm_test_final <- wm_test[, !del_cols_log]\r\n\r\n\r\n##We now split the updated training dataset into a training dataset and a validation dataset. \r\n#This validation dataset will allow us to perform cross validation when developing our model.\r\n\r\ninTrain = createDataPartition(y = training_fewer_cols$classe, p = 0.7, list = FALSE)\r\n\r\nsmall_train = training_fewer_cols[inTrain, ]\r\n\r\nsmall_valid = training_fewer_cols[-inTrain, ]\r\n\r\n#At this point, our dataset contains 54 variables, with the last column containing the 'classe' variable we are trying to \r\n#predict. We begin by looking at the correlations between the variables in our dataset. \r\n#We may want to remove highly correlated predictors from our analysis and replace them with weighted combinations of predictors. This may allow a more \r\n#complete capture of the information available.\r\n\r\ncorMat <- cor(small_train[, -54])\r\n\r\ncorrplot(corMat, order = \"FPC\", method = \"color\", type = \"lower\", tl.cex = 0.8, tl.col = rgb(0, 0, 0))\r\n\r\n    \r\n    ![CorreplotGrid]([url=http://postimg.org/image/ukpfds1nb/][img=http://s15.postimg.org/ukpfds1nb/Correlation_Grid.jpg][/url])\r\n   \r\n\r\n\r\n\r\n#The CorrelationGrid shows the correlation between pairs of the predictors in our dataset. From a high-level perspective darker #blue\r\n#and darker red squares indicate high positive and high negative correlations, respectively. Based on this observation, \r\n#we choose to implement a principal components analysis to produce a set of linearly uncorrelated variables\r\n#to use as our predictors.\r\n\r\n*Principal Components Analysis and Machine Learning*\r\n\r\n#We pre-process our data using a principal component analysis, leaving out the last column ('classe'). After pre-processing,\r\n#we use the 'predict' function to apply the pre-processing to both the training and validation subsets of the original larger \r\n#'training' dataset.\r\n\r\npreProc <- preProcess(small_train[, -54], method = \"pca\", thresh = 0.99)\r\n\r\ntrainPC <- predict(preProc, small_train[, -54])\r\n\r\nvalid_testPC <- predict(preProc, small_valid[, -54])\r\n\r\n\r\n#Next, we train a model using a random forest approach on the smaller training dataset. We chose to specify the use of a \r\n#cross validation method when applying the random forest routine in the 'trainControl()' parameter. Without specifying this, \r\n#the default method (bootstrapping) would have been used. The bootstrapping method seemed to take a lot longer to complete,\r\n#while essentially producing the same level of 'accuracy'.\r\n\r\nmodelFit <- train(small_train$classe ~ ., method = \"rf\", data = trainPC, trControl = trainControl(method = \"cv\",\r\nnumber = 4), importance = TRUE)\r\n    \r\n    \r\n#We now review the relative importance of the resulting principal components of the trained model, 'modelFit'.    \r\n\r\nvarImpPlot(modelFit$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 1, \r\n    main = \"Importance of the Individual Principal Components\")\r\n    \r\n#As you look from the top to the bottom on the y-axis, this plot shows each of the principal components in order from most \r\n#important to least important. The degree of importance is shown on the x-axis–increasing from left to right. \r\n#Therefore, points high and to the right on this graph correspond to those principal components that are especially valuable \r\n#in terms of being able to classify the observed training data.\r\n\r\n**Cross Validation Testing and Out-of-Sample Error Estimate**\r\n\r\n#Call the 'predict' function again so that our trained model can be applied to our cross validation test dataset. \r\n#We can then view the resulting table in the 'confusionMatrix' function's output to see how well the model \r\n#predicted/classified the values in the validation test set (i.e. the 'reference' values)\r\n\r\npred_valid_rf <- predict(modelFit, valid_testPC)\r\nconfus <- confusionMatrix(small_valid$classe, pred_valid_rf)\r\nconfus$table\r\n\r\n#The estimated out-of-sample error is 1.000 minus the model's accuracy, the later of which is provided in the output \r\n#of the confusionmatrix, or more directly via the 'postresample' function.\r\n\r\naccur <- postResample(small_valid$classe, pred_valid_rf)\r\n\r\nmodel_accuracy <- accur[[1]]\r\n\r\nmodel_accuracy\r\n\r\nout_of_sample_error <- 1 - model_accuracy\r\n\r\nout_of_sample_error\r\n\r\n#The estimated accuracy of the model is 98.4% and the estimated out-of-sample error based on our fitted model applied to \r\n#the cross validation dataset is 1.6%.\r\n\r\n\r\n**Predicted Results**\r\n\r\n#Finally, we apply the pre-processing to the original testing dataset, after removing the extraneous column labeled \r\n#'problem_id' (column 54). We then run our model against the testing dataset and display the predicted results.\r\n\r\ntestPC <- predict(preProc, wm_test_final[, -54])\r\n\r\npred_final <- predict(modelFit, testPC)\r\n\r\npred_final\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n    \r\n    \r\n\r\n    \r\n    \r\n    \r\n    \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}